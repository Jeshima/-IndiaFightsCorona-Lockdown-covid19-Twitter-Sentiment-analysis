{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tqdm as tqdm\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "from gensim import summarization\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from math import floor,ceil\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('twt_#ifc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['full_text', 'retweet_count', 'user_followers_count', 'favorite_count',\n",
       "       'created_at', 'pos_score', 'neg_score', 'sent_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['retweet_count', 'user_followers_count', 'favorite_count',\n",
    "       'created_at'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x135974255c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['sent_score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre Processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning( review, remove_stopwords=True):\n",
    "   \n",
    "    \n",
    "\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "   \n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    b=[]\n",
    "    stemmer = english_stemmer \n",
    "    for word in words:\n",
    "        b.append(stemmer.stem(word))\n",
    "\n",
    "    \n",
    "    return(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_Text = []\n",
    "for review in data['full_text']:\n",
    "    clean_Text.append( \" \".join(cleaning(review)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top Word Count In Text(Review)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Count Words Used In Review -- covid                939\n",
      "lockdown             686\n",
      "indiafightscorona    341\n",
      "amp                  188\n",
      "india                187\n",
      "case                 182\n",
      "coronaviru           130\n",
      "new                  124\n",
      "peopl                113\n",
      "state                103\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "Top_Words_Review =pd.Series(' '.join(clean_Text).lower().split()).value_counts()[:10]\n",
    "print (\"Top Count Words Used In Review --\", Top_Words_Review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social: 4.941010542301399\n",
      "like: 4.40692805637114\n",
      "covid: 1.7617319720575568\n",
      "inspir: 6.521460917862246\n",
      "death: 4.324236340526027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df=4, max_features = 10000)\n",
    "vz = vectorizer.fit_transform(clean_Text)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print(\"social: \" + str(tfidf[\"social\"]))\n",
    "print(\"like: \" + str(tfidf[\"like\"]))\n",
    "print(\"covid: \" + str(tfidf[\"covid\"]))\n",
    "print(\"inspir: \" + str(tfidf[\"inspir\"]))\n",
    "print(\"death: \" + str(tfidf[\"death\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compound: 0.6124, \n",
      "neg: 0.0, \n",
      "neu: 0.706, \n",
      "pos: 0.294, \n",
      "whi wait get fine govern take care wear mask maintain social distanc facemask lockdow\n",
      "\n",
      "\n",
      "compound: 0.4404, \n",
      "neg: 0.0, \n",
      "neu: 0.756, \n",
      "pos: 0.244, \n",
      "start monday delhi govern gave relax lockdown howev educ institut\n",
      "\n",
      "\n",
      "compound: 0.3612, \n",
      "neg: 0.0, \n",
      "neu: 0.615, \n",
      "pos: 0.385, \n",
      "meme inform lockdown look like\n",
      "\n",
      "\n",
      "compound: 0.0, \n",
      "neg: 0.0, \n",
      "neu: 1.0, \n",
      "pos: 0.0, \n",
      "lockdown focu reviv economi howev team vijaykarnataka alreadi stimul economi\n",
      "\n",
      "\n",
      "compound: 0.0, \n",
      "neg: 0.0, \n",
      "neu: 1.0, \n",
      "pos: 0.0, \n",
      "hello guy video get look sonam kapoor celebr inspir look priya inturu lockdown unlock priyainturu fashion trend corona covid\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "Senti = SentimentIntensityAnalyzer()\n",
    "sample_review = clean_Text[:5]\n",
    "for sentence in sample_review:\n",
    "    sentence\n",
    "    ss = Senti.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]))\n",
    "    print( sentence) \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " covid\n",
      " india\n",
      " coronaviru\n",
      " pandem\n",
      " indiafightscorona\n",
      "\n",
      "Cluster 1:\n",
      " amp\n",
      " covid\n",
      " talk\n",
      " friend\n",
      " startup\n",
      "\n",
      "Cluster 2:\n",
      " guidelin\n",
      " lockdown\n",
      " open\n",
      " june\n",
      " mall\n",
      "\n",
      "Cluster 3:\n",
      " test\n",
      " covid\n",
      " coronaviru\n",
      " posit\n",
      " app\n",
      "\n",
      "Cluster 4:\n",
      " case\n",
      " covid\n",
      " total\n",
      " new\n",
      " activ\n",
      "\n",
      "Cluster 5:\n",
      " indiafightscorona\n",
      " passeng\n",
      " covid\n",
      " train\n",
      " hand\n",
      "\n",
      "Cluster 6:\n",
      " lockdown\n",
      " state\n",
      " govern\n",
      " happen\n",
      " noth\n",
      "\n",
      "Cluster 7:\n",
      " peopl\n",
      " amp\n",
      " covid\n",
      " age\n",
      " distribut\n",
      "\n",
      "Cluster 8:\n",
      " corona\n",
      " one\n",
      " lockdown\n",
      " like\n",
      " plea\n",
      "\n",
      "Cluster 9:\n",
      " lockdown\n",
      " unlock\n",
      " june\n",
      " till\n",
      " extend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters = 10\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)\n",
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for j in sorted_centroids[i, :5]:\n",
    "        print(' %s' % terms[j])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP.DESKTOP-8R6RI8B.000\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1574/1574 [==============================] - 1s 917us/step - loss: 1.1531 - accuracy: 0.5394\n",
      "Epoch 2/10\n",
      "1574/1574 [==============================] - 1s 667us/step - loss: 0.7748 - accuracy: 0.6728\n",
      "Epoch 3/10\n",
      "1574/1574 [==============================] - 1s 676us/step - loss: 0.5660 - accuracy: 0.8126\n",
      "Epoch 4/10\n",
      "1574/1574 [==============================] - 1s 675us/step - loss: 0.3988 - accuracy: 0.8964\n",
      "Epoch 5/10\n",
      "1574/1574 [==============================] - 1s 664us/step - loss: 0.2746 - accuracy: 0.9352\n",
      "Epoch 6/10\n",
      "1574/1574 [==============================] - 1s 669us/step - loss: 0.1893 - accuracy: 0.9581\n",
      "Epoch 7/10\n",
      "1574/1574 [==============================] - 1s 652us/step - loss: 0.1321 - accuracy: 0.9727\n",
      "Epoch 8/10\n",
      "1574/1574 [==============================] - 1s 657us/step - loss: 0.0938 - accuracy: 0.9803\n",
      "Epoch 9/10\n",
      "1574/1574 [==============================] - 1s 638us/step - loss: 0.0670 - accuracy: 0.98670s - loss: 0.0690 - accu\n",
      "Epoch 10/10\n",
      "1574/1574 [==============================] - 1s 667us/step - loss: 0.0490 - accuracy: 0.9892\n",
      "175/175 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8941591249193463"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Text = data.full_text\n",
    "Ratings = data.sent_score\n",
    "vectorizer = TfidfVectorizer(max_df=.8)\n",
    "vectorizer.fit(Text)\n",
    "def categorize(ratings):\n",
    "    cats = []\n",
    "    for rating in ratings:\n",
    "        v = [0,0,0,0,0]\n",
    "        v[rating-1] = 1\n",
    "        cats.append(v)\n",
    "    return np.array(cats)\n",
    "\n",
    "X = vectorizer.transform(Text).toarray()\n",
    "y = categorize(Ratings.values)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256,input_dim=X_train.shape[1]))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,nb_epoch=10,batch_size=32,verbose=1)\n",
    "model.evaluate(X_test,y_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
